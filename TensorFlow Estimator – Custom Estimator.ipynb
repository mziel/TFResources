{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzielinski/anaconda3/envs/keras/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/mzielinski/anaconda3/envs/keras/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_csv = pd.read_csv(\"/Users/mzielinski/.kaggle/competitions/titanic/train.csv\",\n",
    "                        names = ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
    "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
    "                        skipinitialspace=True, low_memory=False, \n",
    "                        skiprows=1, na_values=[])\n",
    "\n",
    "train_csv.fillna({'PassengerId': '', 'Survived': 0, 'Pclass': 0, 'Name': '', 'Sex': '', 'Age': 0, 'SibSp': 0,\n",
    "       'Parch': 0, 'Ticket': '', 'Fare': 0, 'Cabin': '', 'Embarked': ''}, inplace=True)\n",
    "\n",
    "labels = train_csv[\"Survived\"]\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "features = train_csv[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_age = tf.feature_column.numeric_column(\"Age\", default_value=features[\"Age\"].mean())\n",
    "numeric_fare = tf.feature_column.numeric_column(\"Fare\", default_value=features[\"Fare\"].mean())\n",
    "numeric_features = [numeric_age, numeric_fare]\n",
    "\n",
    "bucketized_age = tf.feature_column.bucketized_column(numeric_age, [0, 10, 20, 30, 40, 50, 60, 100])\n",
    "one_hot_age = tf.feature_column.indicator_column(bucketized_age)\n",
    "categorical_identity_cols = [\"Pclass\", \"SibSp\", \"Parch\"]\n",
    "categorical_identity_features = [\n",
    "    tf.feature_column.categorical_column_with_identity(\n",
    "        key,\n",
    "        len(features[key].unique()) + 1,\n",
    "        0\n",
    "    ) for key in categorical_identity_cols] \n",
    "one_hot_identity_features = [\n",
    "    tf.feature_column.indicator_column(key) for key in categorical_identity_features]\n",
    "categorical_dictionary_cols = [\"Sex\", \"Embarked\"]\n",
    "categorical_dictionary_features = [\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key,\n",
    "        list(features[key].dropna().unique())\n",
    "    ) for key in categorical_dictionary_cols] \n",
    "one_hot_dictionary_features = [\n",
    "    tf.feature_column.indicator_column(key) for key in categorical_dictionary_features]\n",
    "categorical_crossed_age_sex = tf.feature_column.crossed_column(\n",
    "    [bucketized_age, categorical_dictionary_features[0]],\n",
    "    5000)\n",
    "categorical_embedding_crossed_age_sex = tf.feature_column.embedding_column(categorical_crossed_age_sex, 9)\n",
    "categorical_features = one_hot_identity_features + one_hot_dictionary_features + [one_hot_age, categorical_embedding_crossed_age_sex]\n",
    "all_features = numeric_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the read end of the pipeline.\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the following figure shows, pre-made Estimators are **subclasses** of the `tf.estimator.Estimator` base class, while custom Estimators are an **instance** of `tf.estimator.Estimator`:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/custom_estimators/estimator_types.png\" width=600>\n",
    "\n",
    "A **model function** (or `model_fn`) implements the ML algorithm. The only difference between working with pre-made Estimators and custom Estimators is with custom `Estimators`, you must write the model function. The model function we'll use has the following call signature:\n",
    "\n",
    "    def my_model_fn(\n",
    "       features, # This is batch_features from input_fn\n",
    "       labels,   # This is batch_labels from input_fn\n",
    "       mode,     # An instance of tf.estimator.ModeKeys\n",
    "       params):  # Additional configuration\n",
    "\n",
    "The caller may pass `params` to an Estimator's constructor. Any params passed to the constructor are in turn passed on to the `model_fn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "#### Define the input layer\n",
    "\n",
    "The first line of the model_fn calls `tf.feature_column.input_layer` to convert the feature dictionary and `feature_columns` into input for your model, as follows. The line applies the transformations defined by your feature columns, creating the model's input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"feature_columns\": all_features, \"hidden_units\": [10, 10], \"n_classes\": 2}\n",
    "net = tf.feature_column.input_layer(dict(features), params['feature_columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layers\n",
    "\n",
    "The `Layers API` provides a rich set of functions to define all types of hidden layers, including convolutional, pooling, and dropout layers. Here we're simply going to call `tf.layers.dense` to create hidden layers, with dimensions defined by `params['hidden_layers']`.\n",
    "\n",
    "Note that `tf.layers.dense` provides many additional capabilities, including the ability to set a multitude of regularization parameters. For the sake of simplicity, though, we're going to simply accept the default values of the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for units in params['hidden_units']:\n",
    "    net = tf.layers.dense(net, units=units, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer\n",
    "\n",
    "We'll define the output layer by calling `tf.layers.dense` yet again, this time without an activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logits (1 per class).\n",
    "logits = tf.layers.dense(net, params['n_classes'], activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement training, evaluation, and prediction\n",
    "The model function gets invoked whenever someone calls the Estimator's `train`, `evaluate`, or `predict` methods. Recall that the signature for the model function looks like this:\n",
    "\n",
    "    def my_model_fn(\n",
    "       features, # This is batch_features from input_fn\n",
    "       labels,   # This is batch_labels from input_fn\n",
    "       mode,     # An instance of tf.estimator.ModeKeys\n",
    "       params):  # Additional configuration\n",
    "\n",
    "Focus on that third argument, `mode`. As the following table shows, when someone calls train, evaluate, or predict, the Estimator framework invokes your model function with the mode parameter set as follows:\n",
    "\n",
    "    |Estimator method | Estimator Mode   |\n",
    "    |-----------------|------------------|\n",
    "    |train()\t      | ModeKeys.TRAIN   |\n",
    "    |evaluate()       | ModeKeys.EVAL    |\n",
    "    |predict()        | ModeKeys.PREDICT |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict\n",
    "\n",
    "When the Estimator's `predict` method is called, the `model_fn` receives `mode = ModeKeys.PREDICT`. In this case, the model function must return a `tf.estimator.EstimatorSpec` containing the prediction. \n",
    "\n",
    "The prediction dictionary contains everything that your model returns when run in prediction mode. We return that dictionary to the caller via the `predictions` parameter of the `tf.estimator.EstimatorSpec`. The Estimator's `predict` method will yield these dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EstimatorSpec(mode='infer', predictions={'probabilities': <tf.Tensor 'Softmax:0' shape=(891, 2) dtype=float32>, 'class_ids': <tf.Tensor 'strided_slice:0' shape=(891, 1) dtype=int64>, 'logits': <tf.Tensor 'dense_8/BiasAdd:0' shape=(891, 2) dtype=float32>}, loss=None, train_op=None, eval_metric_ops={}, export_outputs=None, training_chief_hooks=(), training_hooks=(), scaffold=<tensorflow.python.training.monitored_session.Scaffold object at 0x120e51a90>, evaluation_hooks=())"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "# Compute predictions.\n",
    "predicted_classes = tf.argmax(logits, 1)\n",
    "if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions = {\n",
    "        'class_ids': predicted_classes[:, tf.newaxis],\n",
    "        'probabilities': tf.nn.softmax(logits),\n",
    "        'logits': logits,\n",
    "    }\n",
    "    out = tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the loss\n",
    "\n",
    "For both training and evaluation we need to calculate the model's loss. This is the objective that will be optimized. \n",
    "We can calculate the loss by calling `tf.losses.sparse_softmax_cross_entropy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss.\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "When the Estimator's `evaluate` method is called, the `model_fn` receives `mode = ModeKeys.EVAL`. In this case, the model function must return a `tf.estimator.EstimatorSpec` containing the model's loss and optionally one or more metrics.\n",
    "\n",
    "Although returning metrics is optional, most custom Estimators do return at least one metric. TensorFlow provides a Metrics module `tf.metrics` to calculate common metrics. The `tf.metrics.accuracy` function compares our predictions against the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics.\n",
    "accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                               predictions=predicted_classes,\n",
    "                               name='acc_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EstimatorSpec` returned for evaluation typically contains the following information:\n",
    "\n",
    "* **loss**, which is the model's loss\n",
    "* **eval_metric_ops**, which is an optional dictionary of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'accuracy': accuracy}\n",
    "tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    out = tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "When the Estimator's `train` method is called, the `model_fn` is called with `mode = ModeKeys.TRAIN`. In this case, the model function must return an `EstimatorSpec` that contains the loss and a training operation.\n",
    "\n",
    "Building the training operation will require an optimizer. We will use `tf.train.AdagradOptimizer` because we're mimicking the `DNNClassifier`, which also uses `Adagrad` by default. The `tf.train` package provides many other optimizers—feel free to experiment with them.\n",
    "\n",
    "The `minimize` method also takes a `global_step` parameter. TensorFlow uses this parameter to count the number of training steps that have been processed (to know when to end a training run). Furthermore, the global_step is essential for TensorBoard graphs to work correctly. Simply call `tf.train.get_global_step` and pass the result to the global_step argument of minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EstimatorSpec` returned for training must have the following fields set:\n",
    "\n",
    "* `loss`, which contains the value of the loss function.\n",
    "* `train_op`, which executes a training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EstimatorSpec(mode='train', predictions={}, loss=<tf.Tensor 'sparse_softmax_cross_entropy_loss/value:0' shape=() dtype=float32>, train_op=<tf.Operation 'Adagrad' type=NoOp>, eval_metric_ops={}, export_outputs=None, training_chief_hooks=(), training_hooks=(), scaffold=<tensorflow.python.training.monitored_session.Scaffold object at 0x120ecc470>, evaluation_hooks=())"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    \"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\n",
    "    # Create three fully connected layers each layer having a dropout\n",
    "    # probability of 0.1.\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # Compute logits (1 per class).\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate custom Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_log_step_count_steps': 100, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_master': '', '_model_dir': '/tmp/tfcustom/', '_task_type': 'worker', '_session_config': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12121c358>, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_service': None}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model,\n",
    "    params={\n",
    "        'feature_columns': all_features,\n",
    "        # Two hidden layers of 10 nodes each.\n",
    "        'hidden_units': [10, 10],\n",
    "        # The model must choose between 2 classes.\n",
    "        'n_classes': 2,\n",
    "    },\n",
    "    model_dir=\"/tmp/tfcustom/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tfcustom/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 4.754282\n",
      "INFO:tensorflow:global_step/sec: 254.687\n",
      "INFO:tensorflow:step = 101, loss = 0.52425635 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.787\n",
      "INFO:tensorflow:step = 201, loss = 0.63963854 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.738\n",
      "INFO:tensorflow:step = 301, loss = 0.6613085 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.736\n",
      "INFO:tensorflow:step = 401, loss = 0.48632666 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.843\n",
      "INFO:tensorflow:step = 501, loss = 0.41039792 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 448.65\n",
      "INFO:tensorflow:step = 601, loss = 0.43381172 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 421.63\n",
      "INFO:tensorflow:step = 701, loss = 0.5299153 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.338\n",
      "INFO:tensorflow:step = 801, loss = 0.41666624 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.83\n",
      "INFO:tensorflow:step = 901, loss = 0.395578 (0.344 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tfcustom/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.45912752.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x121275898>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    steps=1000,\n",
    "    input_fn=lambda : train_input_fn(features, labels, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Estimator\n",
    "Follow the guide [here| https://www.tensorflow.org/tutorials/layers]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
